{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from datetime import date, timedelta, datetime\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import seaborn as sns\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "rootpath = r\"/home/bingo/桌面/sales_forecasting/steel_storage_throughput_prediction\"\n",
    "os.chdir(rootpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selecting data\n",
    "- 加载表格，库存数据集、储户费用数据集、出入库训练集\n",
    "- 预测任务\n",
    "    - 1.按照两大类货品类型（冷卷、热卷），分别预测未来4个周钢铁的周入库量和周出库量（重量）；\n",
    "    - 2.按照两大类货品类型（冷卷、热卷），分别预测未来i天的日入库量和日出库量（重量）。\n",
    "- 只有产品名称是热卷和圆钢为热卷类型，其余均为冷卷类型\n",
    "- 整理目标：仅筛选出包含冷卷和热卷两种货品的数据集，添加新列表示冷卷或热卷"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "field_names = pd.read_csv(r\"01.初赛下载数据/字段说明.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 入库字段说明\n",
    "field_names.tail(9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_train_in = pd.read_csv(r\"01.初赛下载数据/trainData_IN.csv\")\n",
    "df_train_ext = pd.read_csv(r\"01.初赛下载数据/trainData_EXT.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cool_col = {\"type\":\"冷卷\", \"type_id\":0}\n",
    "hot_col = {\"type\":\"热卷\", \"type_id\":1}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_item(dataset, target_col, item_list, new_col=None):\n",
    "    df = pd.DataFrame(columns=dataset.columns)\n",
    "    for item in item_list:\n",
    "        temp = dataset[dataset[target_col]==item]\n",
    "        df = pd.concat([df,temp], axis=0)\n",
    "    if new_col:\n",
    "        for key, item in new_col.items():\n",
    "            df[key] = item\n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hot_in = [\"热卷\",\"圆钢\"]\n",
    "cool_in = set(df_train_in[\"r201i\"].unique()) - set(hot_in)\n",
    "\n",
    "train_in_hot = get_item(df_train_in, \"r201i\", hot_in, hot_col)\n",
    "train_in_cool = get_item(df_train_in, \"r201i\", cool_in, cool_col)\n",
    "train_in_both = pd.concat([train_in_hot,train_in_cool],axis=0)\n",
    "del train_in_hot,train_in_cool"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "hot_ext = [\"热卷\",\"圆钢\"]\n",
    "cool_ext = set(df_train_ext[\"c201i\"].unique()) - set(hot_ext)\n",
    "\n",
    "train_ext_hot = get_item(df_train_ext, \"c201i\", hot_ext, hot_col)\n",
    "train_ext_cool = get_item(df_train_ext, \"c201i\", cool_ext, cool_col)\n",
    "train_ext_both = pd.concat([train_ext_hot, train_ext_cool], axis=0)\n",
    "del train_ext_cool, train_ext_hot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 出入库数据集样本数\n",
    "print(\"train_in samples: \", train_in_both.shape)\n",
    "print(\"train_ext samples: \", train_ext_both.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_in_both.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_in_both.to_csv(\"data/train_in.csv\",index=False)\n",
    "train_ext_both.to_csv(\"data/train_ext.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data\n",
    "- 加载表格：加载提取后的出入库数据集\n",
    "- 整理目标\n",
    "    - 转换为储户id-产品id-日期格式三层索引，做为训练集格式\n",
    "    - 无记录时间记出入库重量为0\n",
    "    - 补齐中断的缺失时间，时间范围为(2014-02-24,2018-01-28)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 入库训练集\n",
    "dtypes = {\n",
    "    \"r2014\":str,\n",
    "    \"r2015\":int,\n",
    "    \"r2019\":float,\n",
    "    \"type_id\":int,\n",
    "}\n",
    "use_cols = [0,1,5,10]\n",
    "# dates = [\"r2021\",\"r2022\"]\n",
    "train_in = pd.read_csv(\"data/train_in.csv\",dtype=dtypes,usecols=use_cols)\n",
    "\n",
    "train_in[\"date\"] = pd.to_datetime(train_in[\"r2014\"].apply(lambda x:x[2:10]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 选择数据列，更改列名\n",
    "select_col = [\"date\",\"r2015\",\"r2019\",\"type_id\"]\n",
    "new_names = [\"date\",\"store_id\",\"sales\", \"type_id\"]\n",
    "train_in = train_in[select_col]\n",
    "\n",
    "train_in.columns = new_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 补全2014-2-24 2018-01-28日期内，缺失的时间及记录\n",
    "temp = train_in.groupby([\"store_id\",\"type_id\",\"date\"])[[\"sales\"]].sum().unstack(level=-1).fillna(0)\n",
    "temp.columns = temp.columns.get_level_values(1)\n",
    "\n",
    "# 2014-02-24为周一，2018-01-28为周日\n",
    "train_in_all = pd.DataFrame(temp, index=temp.index, \n",
    "                            columns=pd.date_range(date(2014,2,24), date(2018,1,28)))\n",
    "train_in_all.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 出库训练集\n",
    "dtypes = {\n",
    "    \"c2014\":str,\n",
    "    \"c2015\":int,\n",
    "    \"c2019\":float,\n",
    "    \"type_id\":int,\n",
    "}\n",
    "use_cols = [0,1,5,10]\n",
    "# dates = [\"r2021\",\"r2022\"]\n",
    "train_ext = pd.read_csv(\"data/train_ext.csv\",dtype=dtypes,usecols=use_cols)\n",
    "\n",
    "train_ext[\"date\"] = pd.to_datetime(train_ext[\"c2014\"].\\\n",
    "                                   apply(lambda x:\"\".join(list(filter(str.isdigit,x))[:8])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "select_col = [\"date\",\"c2015\",\"c2019\",\"type_id\"]\n",
    "\n",
    "train_ext = train_ext[select_col]\n",
    "train_ext.columns = new_names"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "temp = train_ext.groupby([\"store_id\",\"type_id\",\"date\"])[[\"sales\"]].sum().unstack(level=-1).fillna(0)\n",
    "temp.columns = temp.columns.get_level_values(1)\n",
    "\n",
    "train_ext_all = pd.DataFrame(temp, index=temp.index, \n",
    "                             columns=pd.date_range(date(2014,2,24),date(2018,1,28)))\n",
    "train_ext_all.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_in_all.to_csv(\"data/train_in_all.csv\")\n",
    "train_ext_all.to_csv(\"data/train_ext_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data extraction\n",
    "- 特征提取\n",
    "- 滑窗选取数据集\n",
    "- 有大量0数值，稀疏数据\n",
    "- 以4周数据，预测下一周i天数据\n",
    "- 时间点\n",
    "    - 测试集：2018-01-28\n",
    "    - 验证集：2018-01-21\n",
    "    - 训练集：2018-01-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 合并含有大量0值的行\n",
    "def merge_zeros_cols(df, merge_end=500, merge_step=50):\n",
    "    freq = df.columns.freqstr\n",
    "    # 添加一列统计每行0值，用于合并\n",
    "    notzeros = np.count_nonzero(df, axis=1)\n",
    "    merge_list = np.arange(0,merge_end+1,merge_step)\n",
    "    df = df.reset_index()\n",
    "    # 不用合并的行\n",
    "    df_ = df[notzeros>=merge_end].set_index(\"type_id\")\n",
    "    for i, value in enumerate(merge_list[:-1]):\n",
    "        # 待合并行的布尔索引\n",
    "        ind = (notzeros>=value)&(notzeros<merge_list[i+1])\n",
    "        tmp = df.iloc[ind,:]\n",
    "        tmp = tmp.groupby(\"type_id\").sum(axis=0)\n",
    "        tmp[\"store_id\"] = merge_list[i+1]\n",
    "        df_ = pd.concat([df_,tmp],axis=0)\n",
    "#     df_ = df_.drop(\"notzeros\",axis=0)\n",
    "    df_ = df_.reset_index().set_index([\"store_id\",\"type_id\"])\n",
    "    df_.columns = pd.DatetimeIndex(df_.columns,freq=freq)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cnt_zeros(df):\n",
    "    notzeros = np.count_nonzero(df,axis=1)\n",
    "    zeros = df.shape[1]-notzeros\n",
    "    zeros_ratio = zeros/df.shape[1]\n",
    "    pd.Series(zeros_ratio).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_in shape:  (1219, 1463)\n",
      "df_train_ext_shape:  (1213, 1463)\n"
     ]
    }
   ],
   "source": [
    "# 导入数据集，保持稀疏性\n",
    "# converters={\"unit_sales\":lambda x:np.log1p(float(x)) if float(x)>0 else 0}\n",
    "span=30\n",
    "df_train_in = pd.read_csv(\"data/train_in_all.csv\").set_index([\"store_id\",\"type_id\"])/1000\n",
    "df_train_in.columns = pd.to_datetime(df_train_in.columns)\n",
    "df_train_in.columns.name = \"date\"\n",
    "# 窗口平滑\n",
    "df_train_in = df_train_in.ewm(span=span, axis=1).mean()\n",
    "\n",
    "df_train_ext = pd.read_csv(\"data/train_ext_all.csv\").set_index([\"store_id\",\"type_id\"])/1000\n",
    "df_train_ext.columns = pd.to_datetime(df_train_ext.columns)\n",
    "df_train_ext.columns.name = \"date\"\n",
    "# 窗口平滑\n",
    "df_train_ext = df_train_ext.ewm(span=span, axis=1).mean()\n",
    "\n",
    "print(\"df_train_in shape: \", df_train_in.shape)\n",
    "print(\"df_train_ext_shape: \", df_train_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_in_W shape:  (645, 209)\n",
      "df_train_ext_W shape:  (631, 209)\n"
     ]
    }
   ],
   "source": [
    "# 取对数后重采样, 若先重采样再取对数则会预测值过大exp溢出\n",
    "# 150-50,(128,155); 200-100,(87,122);500-100,(37,57)\n",
    "# 有差分：合并->取对数->重采样->差分, 独立做一遍\n",
    "span = 20\n",
    "tmp_in = merge_zeros_cols(df_train_in, 500, 100).transpose()\n",
    "# tmp_in = tmp_in.ewm(span=span,axis=1).mean()\n",
    "df_train_in_transpose = np.log1p(tmp_in)\n",
    "\n",
    "tmp_ext = merge_zeros_cols(df_train_ext, 500, 100).transpose()\n",
    "# tmp_ext = tmp_ext.ewm(span=span, axis=1).mean()\n",
    "df_train_ext_transpose = np.log1p(tmp_ext)\n",
    "\n",
    "# # 对于周数据，无差分：合并->取对数->重采样取均值，置于无差分天数据操作之后\n",
    "# df_train_in_transpose = df_train_in.transpose()\n",
    "# df_train_ext_transpose = df_train_ext.transpose()\n",
    "\n",
    "# 按周重采样\n",
    "df_train_in_W = df_train_in_transpose.resample(\"W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_in_2W = df_train_in_transpose.resample(\"2W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_in_3W = df_train_in_transpose.resample(\"3W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_in_4W = df_train_in_transpose.resample(\"4W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "\n",
    "df_train_ext_W = df_train_ext_transpose.resample(\"W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_ext_2W = df_train_ext_transpose.resample(\"2W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_ext_3W = df_train_ext_transpose.resample(\"3W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_ext_4W = df_train_ext_transpose.resample(\"4W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "\n",
    "# # 差分，去除趋势性，保留初始值\n",
    "# init_in_byWeek = [df_train_in_W.iloc[:,0], df_train_in_2W.iloc[:,0],\\\n",
    "#                   df_train_in_3W.iloc[:,0], df_train_in_4W.iloc[:,0]]\n",
    "# init_ext_byWeek = [df_train_ext_W.iloc[:,0], df_train_ext_2W.iloc[:,0],\\\n",
    "#                   df_train_ext_3W.iloc[:,0], df_train_ext_4W.iloc[:,0]]\n",
    "\n",
    "# df_train_in_W = df_train_in_W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_ext_W = df_train_ext_W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_in_2W = df_train_in_2W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_ext_2W = df_train_ext_2W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_in_3W = df_train_in_3W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_ext_3W = df_train_ext_3W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_in_4W = df_train_in_4W.diff(axis=1).dropna(axis=1)\n",
    "# df_train_ext_4W = df_train_ext_4W.diff(axis=1).dropna(axis=1)\n",
    "print(\"df_train_in_W shape: \",df_train_in_W.shape)\n",
    "print(\"df_train_ext_W shape: \",df_train_ext_W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_in shape:  (645, 1463)\n",
      "df_train_ext_shape:  (631, 1463)\n"
     ]
    }
   ],
   "source": [
    "# 对于天数据，合并->取对数->差分\n",
    "# 先合并数据行\n",
    "span = 180\n",
    "df_train_in = merge_zeros_cols(df_train_in, merge_end=500, merge_step=50*2)#/1000 #100\n",
    "df_train_ext = merge_zeros_cols(df_train_ext, merge_end=500, merge_step=50*2)#/1000 #500,100\n",
    "# df_train_in = df_train_in.ewm(span=span, axis=1).mean()\n",
    "# df_train_ext = df_train_ext.ewm(span=span, axis=1).mean()\n",
    "\n",
    "# 再直接取对数\n",
    "df_train_in = np.log1p(df_train_in)\n",
    "df_train_ext = np.log1p(df_train_ext)\n",
    "\n",
    "# # 最后差分，去除趋势性，保留初始值\n",
    "# init_byDay = [df_train_in.iloc[:,0], df_train_ext.iloc[:,0]]\n",
    "# df_train_in = df_train_in.diff(axis=1).dropna(axis=1)\n",
    "# df_train_ext = df_train_ext.diff(axis=1).dropna(axis=1)\n",
    "print(\"df_train_in shape: \", df_train_in.shape)\n",
    "print(\"df_train_ext_shape: \", df_train_ext.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 探索df_train_in_W的合并方案\n",
    "df_train_in_transpose = np.log1p(merge_zeros_cols(df_train_in, 400, 200).transpose())\n",
    "df_train_in_W = df_train_in_transpose.resample(\"W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "print(df_train_in_W.shape)\n",
    "cnt_zeros(df_train_in_W)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 探索df_train_in的合并区间\n",
    "tmp = merge_zeros_cols(df_train_in, merge_end=500, merge_step=50)\n",
    "print(tmp.shape)\n",
    "cnt_zeros(tmp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cnt_zeros(df_train_ext)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getScaler(df_in, df_ext):\n",
    "    # 数据标准化,按产品id标准化\n",
    "    df_in = df_in.transpose()\n",
    "    df_ext = df_ext.transpose()\n",
    "    \n",
    "    scaler_in = StandardScaler()\n",
    "    scaler_ext = StandardScaler()\n",
    "\n",
    "    scaler_in.fit(df_in)\n",
    "    scaler_ext.fit(df_ext)\n",
    "\n",
    "    df_in = pd.DataFrame(scaler_in.transform(df_in),index=df_in.index, columns=df_in.columns)\n",
    "    df_ext = pd.DataFrame(scaler_ext.transform(df_ext),index=df_ext.index, columns=df_ext.columns)\n",
    "    \n",
    "    df_in = df_in.transpose()\n",
    "    df_ext = df_ext.transpose()\n",
    "    return scaler_in, scaler_ext, df_in, df_ext"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getInvScaler(scalar_in, scaler_ext, df_in, df_ext):\n",
    "    df_in = np.array(df_in)\n",
    "    df_ext = np.array(df_ext)\n",
    "    df_in = scaler_in.inverse_transform(df_in)\n",
    "    df_ext = scaler_ext.inverse_transform(df_ext)\n",
    "    return df_in, df_ext"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 数据标准化\n",
    "scaler_in, scaler_ext, df_train_in, df_train_ext = getScaler(df_train_in, df_train_ext)\n",
    "scaler_in_W, scaler_ext_W, df_train_in_W, df_train_ext_W = getScaler(df_train_in_W, df_train_ext_W)\n",
    "scaler_in_2W, scaler_ext_2W, df_train_in_2W, df_train_ext_2W = getScaler(df_train_in_2W, df_train_ext_2W)\n",
    "scaler_in_3W, scaler_ext_3W, df_train_in_3W, df_train_ext_3W = getScaler(df_train_in_3W, df_train_ext_3W)\n",
    "scaler_in_4W, scaler_ext_4W, df_train_in_4W, df_train_ext_4W = getScaler(df_train_in_4W, df_train_ext_4W)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 按周重采样\n",
    "df_train_in_transpose = np.log1p(df_train_in).transpose()\n",
    "df_train_ext_transpose = np.log1p(df_train_ext).transpose()\n",
    "\n",
    "df_train_in_W = df_train_in_transpose.resample(\"W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_in_2W = df_train_in_transpose.resample(\"2W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_in_3W = df_train_in_transpose.resample(\"3W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_in_4W = df_train_in_transpose.resample(\"4W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "\n",
    "df_train_ext_W = df_train_ext_transpose.resample(\"W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_ext_2W = df_train_ext_transpose.resample(\"2W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_ext_3W = df_train_ext_transpose.resample(\"3W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "df_train_ext_4W = df_train_ext_transpose.resample(\"4W\",closed=\"right\",label=\"right\").mean().transpose()\n",
    "\n",
    "### 维度不一致，代码需要修改，最后得submission部分\n",
    "df_train_in_W = merge_zeros_cols(df_train_in_W, 50, 10)\n",
    "df_train_ext_W = merge_zeros_cols(df_train_ext_W, 50, 10)\n",
    "df_train_in_2W = merge_zeros_cols(df_train_in_2W, merge_end=25, merge_step=5)\n",
    "df_train_ext_2W = merge_zeros_cols(df_train_ext_2W, merge_end=30, merge_step=10)\n",
    "df_train_in_3W = merge_zeros_cols(df_train_in_3W, merge_end=20, merge_step=10)\n",
    "df_train_ext_3W = merge_zeros_cols(df_train_ext_3W, 20,10)\n",
    "df_train_in_4W = merge_zeros_cols(df_train_in_4W, 15, 5)\n",
    "df_train_ext_4W = merge_zeros_cols(df_train_ext_4W,15,5)\n",
    "print(\"df_train_in_W shape: \",df_train_in_W.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 重采样后再取对数\n",
    "df_train_in_W = np.log1p(df_train_in_W)\n",
    "df_train_ext_W = np.log1p(df_train_ext_W)\n",
    "df_train_in_2W = np.log1p(df_train_in_2W)\n",
    "df_train_ext_2W = np.log1p(df_train_ext_2W)\n",
    "df_train_in_3W = np.log1p(df_train_in_3W)\n",
    "df_train_ext_3W = np.log1p(df_train_ext_3W)\n",
    "df_train_in_4W = np.log1p(df_train_in_4W)\n",
    "df_train_ext_4W = np.log1p(df_train_ext_4W)\n",
    "df_train_in = np.log1p(df_train_in)\n",
    "df_train_ext = np.log1p(df_train_ext)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cnt_zeros(df_train_in)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cnt_zeros(df_train_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据子集抽取_byDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_span(df, dt, minus, periods, freq=\"D\"):\n",
    "    return df[pd.date_range(dt-timedelta(days=int(minus)), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_byDay(df, dt, addType=False, is_train=True):\n",
    "    x = {}\n",
    "    n = 11\n",
    "    #1.计算dt时间点一段时间内的出入库情况，总量+均量, 40个特征\n",
    "    for i in np.arange(2,21,2): #np.arange(2,21,2)\n",
    "        tmp = get_span(df, dt, i*7-1, i*7)\n",
    "        x[\"sales_%s_sum\"%(i*7)] = tmp.sum(axis=1).values\n",
    "        x[\"sales_%s_mean\"%(i*7)] = tmp.mean(axis=1).values\n",
    "#         x[\"sum_mean_%s_ratio\"%(i*7)] = x[\"sales_%s_sum\"%(i*7)]/x[\"sales_%s_mean\"%(i*7)]\n",
    "        x[\"sales_%s_sum_decay\"%(i*7)] = \\\n",
    "        (tmp*np.power(0.9,np.arange(i*7)[::-1])).sum(axis=1).values\n",
    "        x[\"sales_%s_mean_decay\"%(i*7)] = \\\n",
    "        (tmp*np.power(0.9,np.arange(i*7)[::-1])).mean(axis=1).values\n",
    "#         x[\"sum_mean_%s_ratio_decay\"%(i*7)] = x[\"sales_%s_sum_decay\"%(i*7)]/x[\"sales_%s_mean_decay\"%(i*7)]\n",
    "    \n",
    "    #2.计算dt时间点一段时间内的出入库次数，有货物天数+无货物天数，20个特征\n",
    "    for i in np.arange(1,n):#np.arange(1,11)\n",
    "        tmp = get_span(df, dt, i*7-1, i*7)\n",
    "        x[\"sales_%s_count\"%(i*7)] = np.count_nonzero(tmp.values, axis=1) \n",
    "        x[\"no_sales_%s_count\"%(i*7)] = i*7 - x[\"sales_%s_count\"%(i*7)]\n",
    "        \n",
    "    #3.计算dt时间点一段时间内产品出入库情况，差值均值、中值、最大值、最小值、标准差，50个特征\n",
    "    for i in np.arange(1,n):#np.arange(1,11)\n",
    "        tmp = get_span(df, dt, i*7-1, i*7)\n",
    "        x[\"diff_%s_mean\"%(i*7)] = tmp.diff(axis=1).mean(axis=1).values\n",
    "#         x[\"diff_%s_mean_decay\"%(i*7)] = \\\n",
    "#         (tmp.diff(axis=1) * np.power(0.9, np.arange(i*7)[::-1])).mean(axis=1).values #新添的\n",
    "        x[\"median_%s\"%(i*7)] = tmp.median(axis=1).values\n",
    "#         x[\"max_%s\"%(i*7)] = tmp.max(axis=1).values\n",
    "#         x[\"min_%s\"%(i*7)] = tmp.min(axis=1).values\n",
    "#         x[\"std_%s\"%(i*7)] = tmp.std(axis=1).values\n",
    "        \n",
    "    #4.dt时间点2周前,差值均值、中值、最大值、最小值、标准差、均值，70个特征\n",
    "    for i in np.arange(1,n):#np.arange(1,11)\n",
    "        tmp = get_span(df, dt+timedelta(days=-14), i*7-1, i*7)\n",
    "        x['diff_%s_mean_2' %(i*7)] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        x['mean_%s_decay_2' %(i*7)] = \\\n",
    "        (tmp * np.power(0.9, np.arange(i*7)[::-1])).mean(axis=1).values #sum\n",
    "        x['mean_%s_2' %(i*7)] = tmp.mean(axis=1).values\n",
    "#         x['median_%s_2' %(i*7)] = tmp.median(axis=1).values\n",
    "#         x['min_%s_2' %(i*7)] = tmp.min(axis=1).values\n",
    "#         x['max_%s_2' %(i*7)] = tmp.max(axis=1).values\n",
    "#         x['std_%s_2' %(i*7)] = tmp.std(axis=1).values\n",
    "    \n",
    "    #5.前一段时间内，第一次出入库距现在天数，最后一次距现在天书， 20个特征\n",
    "    for i in np.arange(1,n):#np.arange(1,11)\n",
    "        tmp = get_span(df, dt, i*7-1, i*7)\n",
    "        x['has_sales_days_in_last_%s'%(i*7)] = (tmp > 0).sum(axis=1).values\n",
    "        x[\"last_sales_days_in_last_%s\"%(i*7)] =\\\n",
    "        i*7 - ((tmp>0)*np.arange(i*7)).max(axis=1).values\n",
    "        x[\"first_sales_days_in_last_%s\"%(i*7)] =\\\n",
    "        ((tmp>0)*np.arange(i*7,0,-1)).max(axis=1).values\n",
    "        \n",
    "    #6.过去n周内同一天的均值、总量, 35个特征\n",
    "    for i in np.arange(7):\n",
    "        x[\"mean_4_dow%s\"%(i)] = get_span(df, dt, 28-i, 4, freq=\"7D\").mean(axis=1).values\n",
    "        x[\"mean_8_dow%s\"%(i)] = get_span(df, dt, 56-i, 8, freq=\"7D\").mean(axis=1).values\n",
    "        x[\"mean_12_dow%s\"%(i)] = get_span(df, dt, 84-i, 12, freq=\"7D\").mean(axis=1).values\n",
    "        x[\"mean_16_dow%s\"%(i)] = get_span(df, dt, 16*7-i, 16, freq=\"7D\").mean(axis=1).values\n",
    "        x[\"mean_20_dow%s\"%(i)] = get_span(df, dt, 140-i, 20, freq=\"7D\").mean(axis=1).values\n",
    "#         x[\"mean_24_dow%s\"%(i)] = get_span(df, dt, 24*7-i, 24, freq=\"7D\").mean(axis=1).values\n",
    "        \n",
    "    #7.提取前30天出入库情况，31个特征\n",
    "    for i in np.arange(0,31):#np.arange(0,31)(n-1)*7\n",
    "        x[\"day_%s\"%(i)] = get_span(df, dt, i, 1).values.ravel()\n",
    "    \n",
    "    if addType:\n",
    "        x = pd.DataFrame(x)\n",
    "        x[\"type\"] = df.reset_index()[\"type_id\"]\n",
    "    else:\n",
    "        x = pd.DataFrame(x)\n",
    "    \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(dt+timedelta(1),periods=7)].values\n",
    "        return x,y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train, y_train = prepare_datasetDay(df_train_in, date(2018,1,28)-timedelta(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "x_train_in :  (64500, 207)\n",
      "y_train_in :  (64500, 7)\n",
      "x_train_ext :  (63100, 207)\n",
      "y_train_ext :  (63100, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "t2018 = date(2018,1,14)\n",
    "num_days = 20*5\n",
    "x_in, y_in = [], []\n",
    "x_ext, y_ext = [], []\n",
    "addType = True\n",
    "\n",
    "for i in range(num_days):\n",
    "    delta = timedelta(days=i)\n",
    "    \n",
    "    x_tmp_in, y_tmp_in = prepare_dataset_byDay(df_train_in, t2018-delta, addType=addType)\n",
    "    x_tmp_ext, y_tmp_ext = prepare_dataset_byDay(df_train_ext, t2018-delta, addType=addType)\n",
    "    \n",
    "    x_in.append(x_tmp_in)\n",
    "    y_in.append(y_tmp_in)\n",
    "    x_ext.append(x_tmp_ext)\n",
    "    y_ext.append(y_tmp_ext)\n",
    "\n",
    "x_train_in = pd.concat(x_in, axis=0)\n",
    "y_train_in = np.concatenate(y_in, axis=0)\n",
    "x_train_ext = pd.concat(x_ext, axis=0)\n",
    "y_train_ext = np.concatenate(y_ext, axis=0)\n",
    "\n",
    "print(\"x_train_in : \",x_train_in.shape)\n",
    "print(\"y_train_in : \", y_train_in.shape)\n",
    "print(\"x_train_ext : \", x_train_ext.shape)\n",
    "print(\"y_train_ext : \", y_train_ext.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 验证集 2018-1-21\n",
    "x_val_in, y_val_in = prepare_dataset_byDay(df_train_in, date(2018,1,21),addType=addType)\n",
    "x_val_ext, y_val_ext = prepare_dataset_byDay(df_train_ext, date(2018,1,21), addType=addType)\n",
    "\n",
    "# 测试集 2018-1-28\n",
    "x_test_in = prepare_dataset_byDay(df_train_in, date(2018,1,28), is_train=False, addType=addType)\n",
    "x_test_ext = prepare_dataset_byDay(df_train_ext, date(2018,1,28), is_train=False, addType=addType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据子集抽取_byWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_span(df, dt, minus, periods, freq=\"D\"):\n",
    "    return df[pd.date_range(dt-timedelta(days=int(minus)), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_byWeek(df, dt, step=7, addType=False, is_train=True):\n",
    "    x = {}\n",
    "    n = 11\n",
    "    freq = df.columns.freqstr\n",
    "    #1.计算dt时间点一段时间内的出入库情况，总量+均量, 40个特征\n",
    "    for i in np.arange(2,21,2):#np.arange(2,21,2)\n",
    "        tmp = get_span(df, dt, i*step, i, freq=freq)\n",
    "        x[\"sales_%s_sum\"%(i)] = tmp.sum(axis=1).values\n",
    "        x[\"sales_%s_mean\"%(i)] = tmp.mean(axis=1).values\n",
    "#         x[\"sum_mean_%s_ratio\"%(i)] = x[\"sales_%s_sum\"%(i)]/x[\"sales_%s_mean\"%(i)]\n",
    "        x[\"sales_%s_sum_decay\"%(i)] = \\\n",
    "        (tmp*np.power(0.9,np.arange(i)[::-1])).sum(axis=1).values\n",
    "        x[\"sales_%s_mean_decay\"%(i)] = \\\n",
    "        (tmp*np.power(0.9,np.arange(i)[::-1])).mean(axis=1).values\n",
    "#         x[\"sum_mean_%s_ratio_decay\"%(i)] = x[\"sales_%s_sum_decay\"%(i)]/x[\"sales_%s_mean_decay\"%(i)]\n",
    "    \n",
    "    #2.计算dt时间点一段时间内的出入库次数，有货物天数+无货物天数，20个特征\n",
    "    for i in np.arange(1,11): #np.arange(1,11)\n",
    "        tmp = get_span(df, dt, i*step, i, freq=freq)\n",
    "        x[\"sales_%s_count\"%(i)] = np.count_nonzero(tmp.values, axis=1) \n",
    "        x[\"no_sales_%s_count\"%(i)] = i - x[\"sales_%s_count\"%(i)]\n",
    "        \n",
    "    #3.计算dt时间点一段时间内产品出入库情况，差值均值、中值、最大值、最小值、标准差，50个特征\n",
    "    for i in np.arange(1,11):#np.arange(1,11)\n",
    "        tmp = get_span(df, dt, i*step, i, freq=freq)\n",
    "        x[\"diff_%s_mean\"%(i)] = tmp.diff(axis=1).mean(axis=1).values\n",
    "#         x[\"diff_%s_mean_decay\"%(i)] = \\\n",
    "#         (tmp.diff(axis=1) * np.power(0.9, np.arange(i)[::-1])).mean(axis=1).values #新添的\n",
    "        x[\"median_%s\"%(i)] = tmp.median(axis=1).values\n",
    "#         x[\"max_%s\"%(i)] = tmp.max(axis=1).values\n",
    "#         x[\"min_%s\"%(i)] = tmp.min(axis=1).values\n",
    "#         x[\"std_%s\"%(i)] = tmp.std(axis=1).values\n",
    "        \n",
    "    #4.dt时间点2周前,差值均值、中值、最大值、最小值、标准差、均值，i0个特征\n",
    "    for i in np.arange(1,11):#np.arange(1,11)\n",
    "        n = step if step%14 or 14%step else 14\n",
    "        tmp = get_span(df, dt+timedelta(days=-n), i*step, i, freq=freq)\n",
    "        x['diff_%s_mean_2' %(i)] = tmp.diff(axis=1).mean(axis=1).values\n",
    "        x['mean_%s_decay_2' %(i)] = \\\n",
    "        (tmp * np.power(0.9, np.arange(i)[::-1])).mean(axis=1).values #sum\n",
    "        x['mean_%s_2' %(i)] = tmp.mean(axis=1).values\n",
    "#         x['median_%s_2' %(i)] = tmp.median(axis=1).values\n",
    "#         x['min_%s_2' %(i)] = tmp.min(axis=1).values\n",
    "#         x['max_%s_2' %(i)] = tmp.max(axis=1).values\n",
    "#         x['std_%s_2' %(i)] = tmp.std(axis=1).values\n",
    "    \n",
    "    #5.前一段时间内，第一次出入库距现在天数，最后一次距现在天书， 20个特征\n",
    "    for i in np.arange(1,11): #np.arange(1,11)\n",
    "        tmp = get_span(df, dt, i*step, i, freq=freq)\n",
    "        x['has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\n",
    "        x[\"last_sales_days_in_last_%s\"%(i)] =\\\n",
    "        i - ((tmp>0)*np.arange(i)).max(axis=1).values\n",
    "        x[\"first_sales_days_in_last_%s\"%(i)] =\\\n",
    "        ((tmp>0)*np.arange(i,0,-1)).max(axis=1).values\n",
    "        \n",
    "    #7.提取前30天出入库情况，31个特征。这各地方不能包括自身\n",
    "    for i in np.arange(1,11):#np.arange(1,11)\n",
    "        x[\"day_%s\"%(i)] = get_span(df, dt, i*step, 1, freq=freq).values.ravel()\n",
    "    \n",
    "    if addType:\n",
    "        x = pd.DataFrame(x)\n",
    "        x[\"type\"] = df.reset_index()[\"type_id\"]\n",
    "    else:\n",
    "        x = pd.DataFrame(x)\n",
    "    \n",
    "    if is_train:\n",
    "        y = df[pd.date_range(dt,periods=1,freq=freq)].values\n",
    "        return x,y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getDatas(df_in, df_ext, dt=\"2018-01-21\", step=7, num_days=int(20), is_train=True, addType=True):\n",
    "    print(\"Preparing dataset...step\",step)\n",
    "    dt = datetime.strptime(dt,\"%Y-%m-%d\")\n",
    "    num_days = num_days\n",
    "    x_in, y_in = [], []\n",
    "    x_ext, y_ext = [], []\n",
    "    for i in range(num_days):\n",
    "        delta = timedelta(days=i*step)\n",
    "        if is_train:\n",
    "            x_tmp_in, y_tmp_in = prepare_dataset_byWeek(df_in, dt-delta, step=step, addType=addType, is_train=is_train)\n",
    "            x_tmp_ext, y_tmp_ext = prepare_dataset_byWeek(df_ext, dt-delta, step=step, addType=addType,is_train=is_train)\n",
    "        else:\n",
    "            x_tmp_in = prepare_dataset_byWeek(df_in, dt-delta, step=step, addType=addType, is_train=is_train)\n",
    "            x_tmp_ext = prepare_dataset_byWeek(df_ext, dt-delta, step=step, addType=addType,is_train=is_train)\n",
    "            y_tmp_in = []\n",
    "            y_tmp_ext = []\n",
    "\n",
    "        x_in.append(x_tmp_in)\n",
    "        y_in.append(y_tmp_in)\n",
    "        x_ext.append(x_tmp_ext)\n",
    "        y_ext.append(y_tmp_ext)\n",
    "\n",
    "    x_train_in = pd.concat(x_in, axis=0)\n",
    "    y_train_in = np.concatenate(y_in, axis=0)\n",
    "    x_train_ext = pd.concat(x_ext, axis=0)\n",
    "    y_train_ext = np.concatenate(y_ext, axis=0)\n",
    "    return x_train_in, y_train_in, x_train_ext, y_train_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_date = ['2018-01-21','2018-01-14', '2018-01-07', '2017-12-31']\n",
    "# steps = [7,14,21,28]\n",
    "# in_list = [df_train_in_W, df_train_in_2W, df_train_in_3W, df_train_in_4W]\n",
    "# ext_list = [df_train_ext_W, df_train_ext_2W, df_train_ext_3W, df_train_ext_4W]\n",
    "\n",
    "# 训练集\n",
    "x_train_in_W, y_train_in_W, x_train_ext_W, y_train_ext_W = getDatas(\n",
    "    df_train_in_W, df_train_ext_W, \"2018-01-21\", 7)\n",
    "x_train_in_2W, y_train_in_2W, x_train_ext_2W, y_train_ext_2W = getDatas(\n",
    "    df_train_in_2W, df_train_ext_2W, \"2018-01-14\", 14)\n",
    "x_train_in_3W, y_train_in_3W, x_train_ext_3W, y_train_ext_3W = getDatas(\n",
    "    df_train_in_3W, df_train_ext_3W, \"2018-01-07\", 21)\n",
    "x_train_in_4W, y_train_in_4W, x_train_ext_4W, y_train_ext_4W = getDatas(\n",
    "    df_train_in_4W, df_train_ext_4W, \"2017-12-31\", 28)\n",
    "print(\"x_train_in_W shape: \", x_train_in_W.shape)\n",
    "print(\"x_train_in_2W shape: \", x_train_in_2W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证集\n",
    "# steps = [7,14,21,28]\n",
    "# in_list = [df_train_in_W, df_train_in_2W, df_train_in_3W, df_train_in_4W]\n",
    "# ext_list = [df_train_ext_W, df_train_ext_2W, df_train_ext_3W, df_train_ext_4W]\n",
    "# # val_date = [\"2018-01-07\",\"2017-12-17\",\"2017-11-26\",\"2017-11-05\"]\n",
    "# val_date = ['2018-01-28', '2018-01-28', '2018-01-28', '2018-01-28']\n",
    "\n",
    "x_val_in_W, y_val_in_W, x_val_ext_W, y_val_ext_W = getDatas(\n",
    "    df_train_in_W, df_train_ext_W, \"2018-01-28\", 7, 1)\n",
    "x_val_in_2W, y_val_in_2W, x_val_ext_2W, y_val_ext_2W = getDatas(\n",
    "    df_train_in_2W, df_train_ext_2W, \"2018-01-28\", 14, 1)\n",
    "x_val_in_3W, y_val_in_3W, x_val_ext_3W, y_val_ext_3W = getDatas(\n",
    "    df_train_in_3W, df_train_ext_3W, \"2018-01-28\", 21, 1)\n",
    "x_val_in_4W, y_val_in_4W, x_val_ext_4W, y_val_ext_4W = getDatas(\n",
    "    df_train_in_4W, df_train_ext_4W, \"2018-01-28\", 28, 1)\n",
    "print(\"x_val_in_W shape: \", x_val_in_W.shape)\n",
    "print(\"x_val_ext_2W shape: \", y_val_ext_2W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试集\n",
    "# steps = [7,14,21,28]\n",
    "# in_list = [df_train_in_W, df_train_in_2W, df_train_in_3W, df_train_in_4W]\n",
    "# ext_list = [df_train_ext_W, df_train_ext_2W, df_train_ext_3W, df_train_ext_4W]\n",
    "# test_date = [\"2018-01-29\"]*4\n",
    "\n",
    "x_test_in_W, y_test_in_W, x_test_ext_W, y_test_ext_W = getDatas(\n",
    "    df_train_in_W, df_train_ext_W, \"2018-02-03\", 7, 1, False)\n",
    "x_test_in_2W, y_test_in_2W, x_test_ext_2W, y_test_ext_2W = getDatas(\n",
    "    df_train_in_2W, df_train_ext_2W, \"2018-02-10\", 14, 1, False)\n",
    "x_test_in_3W, y_test_in_3W, x_test_ext_3W, y_test_ext_3W = getDatas(\n",
    "    df_train_in_3W, df_train_ext_3W, \"2018-02-17\", 21, 1, False)\n",
    "x_test_in_4W, y_test_in_4W, x_test_ext_4W, y_test_ext_4W = getDatas(\n",
    "    df_train_in_4W, df_train_ext_4W, \"2018-02-24\", 28, 1, False)\n",
    "print(\"x_test_in_W shape: \", x_test_in_W.shape)\n",
    "print(\"x_test_in_2W shape: \", x_test_ext_2W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_in_Wall = [x_train_in_W, x_train_in_2W, x_train_in_3W, x_train_in_4W]\n",
    "y_train_in_Wall = np.concatenate(\n",
    "    [y_train_in_W, y_train_in_2W, y_train_in_3W, y_train_in_4W], axis=1)\n",
    "# y_train_in_Wall = [y_train_in_W, y_train_in_2W, y_train_in_3W, y_train_in_4W]\n",
    "x_train_ext_Wall = [x_train_ext_W, x_train_ext_2W,\n",
    "                    x_train_ext_3W, x_train_ext_4W]\n",
    "y_train_ext_Wall = np.concatenate(\n",
    "    [y_train_ext_W, y_train_ext_2W, y_train_ext_3W, y_train_ext_4W], axis=1)\n",
    "# y_train_ext_Wall = [y_train_ext_W, y_train_ext_2W, y_train_ext_3W, y_train_ext_4W]\n",
    "\n",
    "x_val_in_Wall = [x_val_in_W, x_val_in_2W, x_val_in_3W, x_val_in_4W]\n",
    "y_val_in_Wall = np.concatenate(\n",
    "    [y_val_in_W, y_val_in_2W, y_val_in_3W, y_val_in_4W], axis=1)\n",
    "# y_val_in_Wall = [y_val_in_W, y_val_in_2W, y_val_in_3W, y_val_in_4W]\n",
    "x_val_ext_Wall = [x_val_ext_W, x_val_ext_2W, x_val_ext_3W, x_val_ext_4W]\n",
    "y_val_ext_Wall = np.concatenate(\n",
    "    [y_val_ext_W, y_val_ext_2W, y_val_ext_3W, y_val_ext_4W], axis=1)\n",
    "# y_val_ext_Wall = [y_val_ext_W, y_val_ext_2W, y_val_ext_3W, y_val_ext_4W]\n",
    "\n",
    "x_test_in_Wall = [x_test_in_W, x_test_in_2W, x_test_in_3W, x_test_in_4W]\n",
    "x_test_ext_Wall = [x_test_ext_W, x_test_ext_2W, x_test_ext_3W, x_test_ext_4W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法建模_byDay\n",
    "- 细粒度预测(按天预测)，不选择使用传统算法\n",
    "    - xgboost\n",
    "    - cnn+dnn\n",
    "    - lstm\n",
    "- 加大数据规模，重新训练模型 2018-12-06, num_days=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 细粒度-1天1个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params1 = {\n",
    "    \"booster\":\"gblinear\",\n",
    "    \"eta\":0.02,\n",
    "    \"alpha\":0,\n",
    "    \"lambda\":1,\n",
    "    \"max_depth\":10, #10\n",
    "    \"colsample_bytree\":0.8,\n",
    "    \"subsample\":0.8,\n",
    "    \"max_leaves\":15, #10\n",
    "    \"gamma\":0.1, # 0.2\n",
    "    \"min_child_weight\":10,\n",
    "    \"nthread\":4,\n",
    "    \"objective\":\"reg:linear\",\n",
    "    \"eval_metric\":\"rmse\",\n",
    "    \"silent\":1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "step 1\n",
      "2018-12-15 15:16:21.315090\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.34807\tdval-rmse:0.347507\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[26]\tdtrain-rmse:0.024772\tdval-rmse:0.019027\n",
      "\n",
      "==================================================\n",
      "step 2\n",
      "2018-12-15 15:16:52.379954\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.348057\tdval-rmse:0.347317\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[26]\tdtrain-rmse:0.025091\tdval-rmse:0.018866\n",
      "\n",
      "==================================================\n",
      "step 3\n",
      "2018-12-15 15:17:20.685680\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.348005\tdval-rmse:0.34717\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[27]\tdtrain-rmse:0.025303\tdval-rmse:0.018552\n",
      "\n",
      "==================================================\n",
      "step 4\n",
      "2018-12-15 15:17:49.332013\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.347996\tdval-rmse:0.347291\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[27]\tdtrain-rmse:0.025707\tdval-rmse:0.018379\n",
      "\n",
      "==================================================\n",
      "step 5\n",
      "2018-12-15 15:18:15.247141\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.348033\tdval-rmse:0.347312\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "[500]\tdtrain-rmse:0.021361\tdval-rmse:0.018232\n",
      "[1000]\tdtrain-rmse:0.021181\tdval-rmse:0.017971\n",
      "[1500]\tdtrain-rmse:0.021143\tdval-rmse:0.017914\n",
      "[1999]\tdtrain-rmse:0.021118\tdval-rmse:0.017877\n",
      "==================================================\n",
      "step 6\n",
      "2018-12-15 15:20:50.569185\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.348025\tdval-rmse:0.34763\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "[500]\tdtrain-rmse:0.021988\tdval-rmse:0.01851\n",
      "[1000]\tdtrain-rmse:0.021813\tdval-rmse:0.018272\n",
      "[1500]\tdtrain-rmse:0.021776\tdval-rmse:0.018221\n",
      "[1999]\tdtrain-rmse:0.021751\tdval-rmse:0.018189\n",
      "==================================================\n",
      "step 7\n",
      "2018-12-15 15:23:25.925127\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.348019\tdval-rmse:0.347906\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[27]\tdtrain-rmse:0.02694\tdval-rmse:0.019309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 一天一个模型\n",
    "boosting_round = [500*2,1000,0,0]#[500*2,1000*2,2000,4000]\n",
    "MAX_ROUNDS = np.sum(boosting_round)\n",
    "etas = [0.01, 0.005, 0.001, 0.0001]\n",
    "eta_ = np.ones(MAX_ROUNDS)\n",
    "eta_list = [np.tile(eta, rounds) for rounds, eta in zip(boosting_round, etas)]\n",
    "eta_list = np.concatenate(eta_list, axis=0)\n",
    "\n",
    "dtest = xgb.DMatrix(x_test_ext.values)\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "if not os.path.exists(r\"./model/model1\"):\n",
    "    os.mkdir(r\"./model/model1\")\n",
    "        \n",
    "for i in range(7):\n",
    "    print(\"=\"*50)\n",
    "    print(\"step %d\"%(i+1))\n",
    "    print(datetime.now())\n",
    "    print(\"=\"*50)\n",
    "    dtrain = xgb.DMatrix(\n",
    "        data=np.concatenate([x_train_in.values,x_train_ext.values],axis=0),\n",
    "        label=np.concatenate([y_train_in[:,i], y_train_ext[:,i]], axis=0)\n",
    "    )\n",
    "    \n",
    "    dval = xgb.DMatrix(\n",
    "        data=np.concatenate([x_val_in.values,x_val_ext.values], axis=0),\n",
    "        label=np.concatenate([y_val_in[:,i], y_val_ext[:,i]], axis=0)\n",
    "    )\n",
    "    \n",
    "    bst = xgb.train(\n",
    "        params1,\n",
    "        dtrain,\n",
    "        num_boost_round=MAX_ROUNDS,\n",
    "        evals=[(dtrain,\"dtrain\"),(dval,\"dval\")],\n",
    "        early_stopping_rounds=300,\n",
    "        verbose_eval=500,\n",
    "        callbacks=[xgb.callback.reset_learning_rate(list(eta_list))]\n",
    "    )\n",
    "\n",
    "    bst.save_model(r\"./model/model1/bst_byday\"+str(i+1)+\".model\")\n",
    "    \n",
    "    val_pred.append(bst.predict(dval))\n",
    "    test_pred.append(bst.predict(dtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法建模_byWeek\n",
    "- 粗粒度预测，以周为单位\n",
    "- 4周4个模型，预测第一周可收敛，预测第二、三、四周完全不收敛。2018-12-04\n",
    "- 换种思路：一个模型直接预测2周，3周，4周，做差分得到每周的数目。2018-12-05\n",
    "    - 收敛，但是预测结果，特别大，10^20量级都有。醉了。\n",
    "    - 可能是后期包含多周得样本占多数，所以预测结果偏大\n",
    "- 换种思路2.0：4个模型分别预测前1周，前2周，前3周，前4周。2018-12-05\n",
    "    - 累积预测后，计算差分为每周预测值\n",
    "    - 数据总体先求对数，再以每周均值为回归目标\n",
    "    - 若以总量为回归目标，不收敛\n",
    "- 思路2.0可以，现增加数据量重新训练，num_days=30 2018-12-06"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 原始学习率\n",
    "# boosting_round = [500*4,1000*3,2000*2,4000*2]\n",
    "# MAX_ROUNDS = np.sum(boosting_round)\n",
    "# etas = [0.02, 0.01, 0.005, 0.001]\n",
    "# eta_ = np.ones(MAX_ROUNDS)\n",
    "# eta_list = [np.tile(eta, rounds) for rounds, eta in zip(boosting_round, etas)]\n",
    "# eta_list = np.concatenate(eta_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 粗粒度-1周1个模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Training and Predicting models...\")\n",
    "params_w = {\n",
    "    \"booster\":\"gblinear\",\n",
    "    \"eta\":0.02,\n",
    "    \"alpha\":0,\n",
    "    \"lambda\":1,\n",
    "    \"max_depth\":5,\n",
    "    \"colsample_bytree\":0.8,\n",
    "    \"subsample\":0.8,\n",
    "    \"max_leaves\":10,\n",
    "    \"gamma\":0.2,\n",
    "    \"min_child_weight\":10,\n",
    "    \"nthread\":4,\n",
    "    \"objective\":\"reg:linear\",\n",
    "    \"eval_metric\":\"mae\"\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 一天一个模型\n",
    "boosting_round = [500*4,1000*3,2000*2,4000*2]\n",
    "MAX_ROUNDS = np.sum(boosting_round)\n",
    "etas = [0.02, 0.01, 0.005, 0.001]\n",
    "eta_ = np.ones(MAX_ROUNDS)\n",
    "eta_list = [np.tile(eta, rounds) for rounds, eta in zip(boosting_round, etas)]\n",
    "eta_list = np.concatenate(eta_list, axis=0)\n",
    "\n",
    "rows_in = 1196*20\n",
    "rows_ext = 1193*20\n",
    "dtest_w = xgb.DMatrix(x_test_ext_week.values[:1193])\n",
    "val_pred_w = []\n",
    "test_pred_w = []\n",
    "for i in range(1):\n",
    "    print(\"=\"*50)\n",
    "    print(\"step %d\"%(i+1))\n",
    "    print(\"=\"*50)\n",
    "    dtrain_w = xgb.DMatrix(\n",
    "        data=np.concatenate([x_train_in_week.values[:rows_in],x_train_ext_week.values[:rows_ext]],axis=0),\n",
    "        label=np.concatenate([y_train_in_week[:rows_in,i], y_train_ext_week[:rows_ext,i]], axis=0)\n",
    "    )\n",
    "    \n",
    "    dval_w = xgb.DMatrix(\n",
    "        data=np.concatenate([x_val_in_week.values[:rows_in],x_val_ext_week.values[:rows_ext]], axis=0),\n",
    "        label=np.concatenate([y_val_in_week[:rows_in,i], y_val_ext_week[:rows_ext,i]], axis=0)\n",
    "    )\n",
    "    \n",
    "    bst_w = xgb.train(\n",
    "        params_w,\n",
    "        dtrain_w,\n",
    "        num_boost_round=MAX_ROUNDS,\n",
    "        evals=[(dtrain_w,\"dtrain\"),(dval_w,\"dval\")],\n",
    "        early_stopping_rounds=1000,\n",
    "        verbose_eval=100,\n",
    "        callbacks=[xgb.callback.reset_learning_rate(list(eta_list))]\n",
    "    )\n",
    "    bst_w.save_model(r\"./model/model1/bst_byweek\"+str(i+1)+\".model\")\n",
    "    \n",
    "    val_pred_w.append(bst_w.predict(dval_w))\n",
    "    test_pred_w.append(bst_w.predict(dtest_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 粗粒度-4个累积模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Predicting models...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and Predicting models...\")\n",
    "params_w4 = {\n",
    "    \"booster\":\"gblinear\",\n",
    "    \"eta\":0.02,\n",
    "    \"alpha\":0,\n",
    "    \"lambda\":1,\n",
    "    \"max_depth\":8, #5\n",
    "    \"colsample_bytree\":0.8,\n",
    "    \"subsample\":0.8,\n",
    "    \"max_leaves\":15, #10\n",
    "    \"gamma\":0.1, #0.2\n",
    "    \"min_child_weight\":10,\n",
    "    \"nthread\":4,\n",
    "    \"objective\":\"reg:linear\",\n",
    "    \"eval_metric\":\"rmse\",\n",
    "    \"silent\":1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "step 1\n",
      "2018-12-15 15:23:51.303434\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.352924\tdval-rmse:0.352285\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[119]\tdtrain-rmse:0.027026\tdval-rmse:0.023671\n",
      "\n",
      "==================================================\n",
      "step 2\n",
      "2018-12-15 15:23:55.688128\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.353118\tdval-rmse:0.352598\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[115]\tdtrain-rmse:0.028519\tdval-rmse:0.03132\n",
      "\n",
      "==================================================\n",
      "step 3\n",
      "2018-12-15 15:23:59.379998\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.353732\tdval-rmse:0.352607\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[148]\tdtrain-rmse:0.029582\tdval-rmse:0.035809\n",
      "\n",
      "==================================================\n",
      "step 4\n",
      "2018-12-15 15:24:02.984326\n",
      "==================================================\n",
      "[0]\tdtrain-rmse:0.357327\tdval-rmse:0.352951\n",
      "Multiple eval metrics have been passed: 'dval-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until dval-rmse hasn't improved in 300 rounds.\n",
      "Stopping. Best iteration:\n",
      "[133]\tdtrain-rmse:0.032589\tdval-rmse:0.040011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boosting_round = [500*2,1000,0,0] #[500*2,1000*2,2000*2,4000*2]\n",
    "MAX_ROUNDS = np.sum(boosting_round)\n",
    "etas = [0.01, 0.005, 0.001, 0.0001] #[0.02, 0.01, 0.005, 0.001]\n",
    "eta_ = np.ones(MAX_ROUNDS)\n",
    "eta_list = [np.tile(eta, rounds) for rounds, eta in zip(boosting_round, etas)]\n",
    "eta_list = np.concatenate(eta_list, axis=0)\n",
    "\n",
    "# MAX_ROUNDS = 2000\n",
    "val_pred_w4 = []\n",
    "test_pred_w4 = []\n",
    "\n",
    "if not os.path.exists(r\"./model/model1\"):\n",
    "        os.mkdir(r\"./model/model1\")\n",
    "        \n",
    "for i in range(4):\n",
    "    print(\"=\"*50)\n",
    "    print(\"step %d\"%(i+1))\n",
    "    print(datetime.now())\n",
    "    print(\"=\"*50)\n",
    "    dtrain_w4 = xgb.DMatrix(\n",
    "        data=np.concatenate([x_train_in_Wall[i].values, x_train_ext_Wall[i].values],axis=0),\n",
    "        label=np.concatenate([y_train_in_Wall[:,i], y_train_ext_Wall[:,i]], axis=0)\n",
    "    )\n",
    "    \n",
    "    dval_w4 = xgb.DMatrix(\n",
    "        data=np.concatenate([x_val_in_Wall[i].values,x_val_ext_Wall[i].values], axis=0),\n",
    "        label=np.concatenate([y_val_in_Wall[:,i], y_val_ext_Wall[:,i]], axis=0)\n",
    "    )\n",
    "    \n",
    "    dtest_w4 = xgb.DMatrix(x_test_ext_Wall[i].values)\n",
    "    \n",
    "    bst_w4 = xgb.train(\n",
    "        params_w4,\n",
    "        dtrain_w4,\n",
    "        num_boost_round=MAX_ROUNDS,\n",
    "        evals=[(dtrain_w4,\"dtrain\"),(dval_w4,\"dval\")],\n",
    "        early_stopping_rounds=300,\n",
    "        verbose_eval=1000,\n",
    "        callbacks=[xgb.callback.reset_learning_rate(list(eta_list))]\n",
    "    )\n",
    "    \n",
    "    bst_w4.save_model(r\"./model/model1/bst_byweek\"+str(i+1)+\".model\")\n",
    "    \n",
    "    \n",
    "    val_pred_w4.append(bst_w4.predict(dval_w4))\n",
    "#     test_pred_w4.append(bst_w4.predict(dtest_w4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法评估标准"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPrediction(preds, inds, steps=[4,4,7,7]):\n",
    "    # preds=[week_ext, week_in, day_ext, day_in]\n",
    "    stack = []\n",
    "    for ind,pred,step in zip(inds, preds, steps):\n",
    "        pred = np.array(pred).transpose()\n",
    "        tmp = pd.DataFrame(np.expm1(pred), index=ind)\n",
    "        tmp = tmp.reset_index().drop(\"store_id\",axis=1)\n",
    "        tmp = tmp.groupby(\"type_id\").sum().values[::-1]\n",
    "        tmp = tmp.reshape(2*step)\n",
    "        stack.append(tmp)\n",
    "    stack_pred = np.concatenate(stack, axis=0)\n",
    "    return stack_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getSMAPE(label, pred, inds):\n",
    "    M = 800\n",
    "    t = getPrediction(label, inds)*10000\n",
    "    p = getPrediction(pred, inds)*10000\n",
    "    print((t-p)/t)\n",
    "    smape = np.mean(np.abs((t - p)/(np.maximum((t+p)/2,M))))\n",
    "    mape = np.mean(np.abs((t-p)/t))\n",
    "#     mse = np.mean(np.power(t-p,2))\n",
    "#     mae = np.mean(np.abs(t-p))\n",
    "    score = (2-smape)*50\n",
    "    print(\"get MAPE %.4f\"%(mape))\n",
    "#     print(\"get MSE %.4f\"%(mse))\n",
    "#     print(\"get MAE %.4f\"%(mae))\n",
    "    print(\"get SMAPW %.4f\"%(smape))\n",
    "    print(\"get score %.4f\"%(score))\n",
    "    return smape,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getLastW(data):\n",
    "    d1 = np.array(data).squeeze().transpose()\n",
    "    # 先差分\n",
    "    d2 = np.expm1(d1)*np.array([7,14,21,28]) #d2 = np.expm1(d1)*np.array([7,14,21,28])\n",
    "    d3 = np.diff(d2, axis=1)\n",
    "#     d3[d3<0] = 0\n",
    "    d2[:,1:]= d3\n",
    "    d2 = np.clip(d2,0,None)\n",
    "    res = np.log1p(d2).transpose()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 无标准化\n",
    "loc1 = df_train_in.shape[0]\n",
    "loc2 = df_train_in_W.shape[0]\n",
    "pred_val_in, pred_val_ext = np.split(np.array(val_pred),np.array([loc1]),axis=1) #1196\n",
    "pred_val_in_W, pred_val_ext_W = np.split(np.array(val_pred_w4),np.array([loc2]),axis=1) # 1196\n",
    "\n",
    "pred_val_in_W = getLastW(pred_val_in_W)\n",
    "pred_val_ext_W = getLastW(pred_val_ext_W)\n",
    "\n",
    "pred = [pred_val_ext_W, pred_val_in_W, pred_val_ext, pred_val_in]\n",
    "label = [y_val_ext_Wall.transpose(), y_val_in_Wall.transpose(), y_val_ext.transpose(), y_val_in.transpose()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 有标准化\n",
    "pred_val_in, pred_val_ext = np.split(np.array(val_pred),np.array([1196]),axis=1)\n",
    "pred_val_in_W, pred_val_ext_W = np.split(np.array(val_pred_w4),np.array([1196]),axis=1)\n",
    "pred_val_in, pred_val_ext = getInvScaler(scaler_in, scaler_ext, pred_val_in, pred_val_ext)\n",
    "s_in = [scaler_in_W, scaler_in_2W, scaler_in_3W, scaler_in_4W]\n",
    "s_ext = [scaler_ext_W, scaler_ext_2W, scaler_ext_3W, scaler_ext_4W]\n",
    "for i in range(4):\n",
    "    pred_val_in_W[i,:], pred_val_ext_W[i,:] = getInvScaler(s_in[i], s_ext[i], pred_val_in_W[i,:], pred_val_ext_W[i,:])\n",
    "    \n",
    "pred_val_in_W = getLastW(pred_val_in_W)\n",
    "pred_val_ext_W = getLastW(pred_val_ext_W)\n",
    "\n",
    "pred = [pred_val_ext_W, pred_val_in_W, pred_val_ext, pred_val_in]\n",
    "label = [y_val_ext_Wall.transpose(), y_val_in_Wall.transpose(), y_val_ext.transpose(), y_val_in.transpose()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.70972362 -6.37014042 -4.32293423 -5.52153552 -5.96404952 -5.74667175\n",
      " -4.30010267 -5.57621819 -6.95309891 -6.84031797 -4.86114917 -5.92072765\n",
      " -6.1356374  -5.78095804 -4.29968574 -5.45108868 -0.0830624  -0.08981472\n",
      " -0.07431601 -0.11210022 -0.11046372 -0.17022486 -0.24004695 -0.08074063\n",
      " -0.0514349  -0.03244491 -0.0306132   0.01456307  0.01112593 -0.03980805\n",
      " -0.07274648 -0.09329705 -0.11296971 -0.15452906 -0.17610106 -0.22529993\n",
      " -0.30463069 -0.13220416 -0.11144526 -0.09522382 -0.08854851 -0.01803336\n",
      " -0.00964606 -0.06971257]\n",
      "get MAPE 2.1263\n",
      "get SMAPW 0.5947\n",
      "get score 70.2673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.59465436279175787, 70.267281860412112)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inds = [df_train_ext_W.index, df_train_in_W.index, df_train_ext.index, df_train_in.index]\n",
    "getSMAPE(label, pred, inds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "getSMAPE(label, pred, inds)\n",
    "[ -5.99256231  -7.69344707  -4.17504688  -6.80443383  -4.12224363\n",
    "  -4.30322669  -2.48549491  -4.79728148 -10.93371034  -7.73865082\n",
    "  -6.3931309   -8.65377948  -4.6747169   -6.43144984  -3.91081736\n",
    "  -5.55797835   0.7970883    0.10126773   0.48259292  -0.44727684\n",
    "   0.24095798   0.21796051   0.36316924   0.6299625    0.45957464\n",
    "   0.41351627   0.22203725   0.53422988   0.62709017   0.2335896\n",
    "   0.35370784  -0.04725195   0.12110368  -0.78172     -0.18175127\n",
    "  -0.04886831  -5.92682664   0.41431963   0.44994367   0.44247119\n",
    "   0.37142095   0.65195226   0.63110995  -0.75734294]\n",
    "get MAPE 2.5368\n",
    "get SMAPW 0.8632\n",
    "get score 56.8407"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prediction(test, path=\"./model/model1\",mode=\"wd\"):\n",
    "    # test=[test_ext_w, test_in_w, test_ext_day, test_in_day]\n",
    "    # test=[W, W2, W3, W4]\n",
    "    if mode==\"wd\":\n",
    "        week_ext, week_in, day_ext, day_in = [], [], [], []\n",
    "        results = [week_ext, week_in, day_ext, day_in]\n",
    "        bases = [\"bst_byweek%s.model\",\"bst_byweek%s.model\",\"bst_byday%s.model\",\"bst_byday%s.model\"]\n",
    "        steps = [0,0,7,7]\n",
    "        for step, res, base, data in zip(steps, results, bases, test):\n",
    "            dtest = xgb.DMatrix(data.values)\n",
    "            for i in range(step):\n",
    "                model = os.path.join(path, base%(i+1))\n",
    "                print(model)\n",
    "                bst = xgb.Booster()\n",
    "                bst.load_model(model)\n",
    "                res.append(bst.predict(dtest))\n",
    "    else:\n",
    "        W, W2, W3, W4 = [], [], [], []\n",
    "        results = [W, W2, W3, W4]\n",
    "        bases = [\"bst_byweek%s.model\"]*4\n",
    "        steps = range(4)\n",
    "        for step, res, base, data in zip(steps, results, bases, test):\n",
    "            dtest = xgb.DMatrix(data.values)\n",
    "            model = os.path.join(path, base%(step+1))\n",
    "            print(model)\n",
    "            bst = xgb.Booster()\n",
    "            bst.load_model(model)\n",
    "            res.append(bst.predict(dtest))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def makeSubmission(preds, inds, steps=[4,4,7,7], filename=\"./submission/result.csv\"):\n",
    "    # preds=[week_ext, week_in, day_ext, day_in]\n",
    "    stack = []\n",
    "    for ind,pred,step in zip(inds, preds, steps):\n",
    "        pred = np.array(pred).transpose()\n",
    "        tmp = pd.DataFrame(np.expm1(pred), index=ind)\n",
    "        tmp = tmp.reset_index().drop(\"store_id\",axis=1)\n",
    "        tmp = tmp.groupby(\"type_id\").sum().values[::-1]\n",
    "        tmp = tmp.reshape(2*step)\n",
    "        stack.append(tmp)\n",
    "    stack_pred = np.concatenate(stack, axis=0)\n",
    "    stack_pred = stack_pred*1000\n",
    "    result = pd.read_csv(\"./submission/result_org.csv\")\n",
    "    result[\"VALUE\"] = stack_pred\n",
    "    result.to_csv(filename,float_format=\"%.4f\",index=False)\n",
    "    print(\"done!\")\n",
    "    return stack_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def invDiff(initValues, diffValues, predValues):\n",
    "    n = len(initValues)\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        initv = initValues[i].values.reshape(-1,1)\n",
    "        diffv = diffValues[i].values\n",
    "        predv = np.array(predValues[i]).transpose()\n",
    "        length = predv.shape[1]\n",
    "        tmp = np.concatenate([initv, diffv, predv],axis=1)\n",
    "        # 先求和再exp\n",
    "        step1 = np.cumsum(tmp, axis=1)\n",
    "        step2 = step1[:,-length:]\n",
    "        res.append(step2.transpose())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1周/天1个模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test = [x_test_ext_week[:1193], x_test_in_week[:1196], x_test_ext, x_test_in]\n",
    "\n",
    "results = prediction(test)\n",
    "\n",
    "inds = [df_train_ext_W.index, df_train_in_W.index, df_train_ext.index, df_train_in.index]\n",
    "\n",
    "pred1 = makeSubmission(results, inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4周累积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model/model1/bst_byweek1.model\n",
      "./model/model1/bst_byweek2.model\n",
      "./model/model1/bst_byweek3.model\n",
      "./model/model1/bst_byweek4.model\n",
      "./model/model1/bst_byweek1.model\n",
      "./model/model1/bst_byweek2.model\n",
      "./model/model1/bst_byweek3.model\n",
      "./model/model1/bst_byweek4.model\n",
      "./model/model1/bst_byday1.model\n",
      "./model/model1/bst_byday2.model\n",
      "./model/model1/bst_byday3.model\n",
      "./model/model1/bst_byday4.model\n",
      "./model/model1/bst_byday5.model\n",
      "./model/model1/bst_byday6.model\n",
      "./model/model1/bst_byday7.model\n",
      "./model/model1/bst_byday1.model\n",
      "./model/model1/bst_byday2.model\n",
      "./model/model1/bst_byday3.model\n",
      "./model/model1/bst_byday4.model\n",
      "./model/model1/bst_byday5.model\n",
      "./model/model1/bst_byday6.model\n",
      "./model/model1/bst_byday7.model\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "x_test_in_Wall = [x_test_in_W, x_test_in_2W, x_test_in_3W, x_test_in_4W]\n",
    "x_test_ext_Wall = [x_test_ext_W, x_test_ext_2W, x_test_ext_3W, x_test_ext_4W]\n",
    "test = [x_test_ext_W, x_test_in_W, x_test_ext, x_test_in]\n",
    "\n",
    "mpath = './model/model1'\n",
    "res_ext_W = prediction(x_test_ext_Wall,path=mpath,mode=\"w\")\n",
    "res_in_W = prediction(x_test_in_Wall,path=mpath, mode=\"w\")\n",
    "res = prediction(test,path=mpath)\n",
    "\n",
    "# # -----------------------有差分时使用-----------------------------\n",
    "# # 周数据还原，去差分->求各周->取exp\n",
    "# # init_in_byWeek, init_ext_byWeek\n",
    "# diff_in_byWeek = [df_train_in_W, df_train_in_2W, df_train_in_3W, df_train_in_4W]\n",
    "# diff_ext_byWeek = [df_train_ext_W, df_train_ext_2W, df_train_ext_3W, df_train_ext_4W]\n",
    "# diff_byDay = [df_train_in, df_train_ext]\n",
    "# res_ext_W = invDiff(init_ext_byWeek, diff_ext_byWeek, res_ext_W)\n",
    "# res_in_W = invDiff(init_in_byWeek, diff_in_byWeek, res_in_W)\n",
    "# res = invDiff(init_byDay, diff_byDay, [res[3], res[2]])\n",
    "\n",
    "# # 求各周\n",
    "# res1 = getLastW(res_ext_W)\n",
    "# res2 = getLastW(res_in_W)\n",
    "# results = [res1,res2,res[1],res[0]]\n",
    "\n",
    "# inds = [df_train_ext_W.index, df_train_in_W.index, df_train_ext.index, df_train_in.index]\n",
    "\n",
    "# # 取exp\n",
    "# pred1 = makeSubmission(results, inds)\n",
    "\n",
    "# # 标准化缩放\n",
    "# res[3], res[2] = getInvScaler(scaler_in, scaler_ext, res[3], res[2])\n",
    "# s_in = [scaler_in_W, scaler_in_2W, scaler_in_3W, scaler_in_4W]\n",
    "# s_ext = [scaler_ext_W, scaler_ext_2W, scaler_ext_3W, scaler_ext_4W]\n",
    "# for i in range(4):\n",
    "#     res_in_W[i], res_ext_W[i] = getInvScaler(s_in[i], s_ext[i], res_in_W[i], res_ext_W[i])\n",
    "\n",
    "# -----------------------无差分时使用-------------------------------\n",
    "# 求各周\n",
    "res1 = getLastW(res_ext_W)\n",
    "res2 = getLastW(res_in_W)\n",
    "results = [res1,res2,res[2],res[3]]\n",
    "\n",
    "inds = [df_train_ext_W.index, df_train_in_W.index, df_train_ext.index, df_train_in.index]\n",
    "\n",
    "# 取exp\n",
    "pred1 = makeSubmission(results, inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "resbest = pd.read_csv(r\"./submission/result7781.csv\",index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4973.98741806, -2206.91515395,  2013.0555274 ,  2413.57478776,\n",
       "       -3762.43909355, -2740.60012825,  4449.80139139,  1729.50725537,\n",
       "       -4868.71065402, -3016.48252795,  2634.27910608,   563.0655478 ,\n",
       "       -3480.23898354, -2416.70749963,  5138.73314029,   134.97734347,\n",
       "       -1161.65157805,  -789.49597207,  -897.57275254,  -928.04450859,\n",
       "       -1017.14130179, -1676.76851503, -1809.7297691 ,  -990.78509912,\n",
       "        -592.99847942,  -703.48027135,  -639.78293851,  -525.38199845,\n",
       "       -1258.21499231, -1751.93502377, -1182.10677028,  -998.54803184,\n",
       "       -1080.77361972, -1055.94542234, -1092.96080389, -1443.81547574,\n",
       "       -1489.74848915, -1104.634283  ,  -815.1598202 ,  -918.13759143,\n",
       "        -886.91042432,  -752.23018569, -1244.91312875, -1678.33854747])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resbest.values.reshape(-1)-pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32163.5231972\n",
      "60373.7030029\n",
      "30105.588913\n",
      "60987.6403809\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(pred1[-28:-21]))\n",
    "print(np.sum(pred1[-21:-14]))\n",
    "print(np.sum(pred1[-14:-7]))\n",
    "print(np.sum(pred1[-7:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31938.57211806,  31174.74065395,  23335.5192726 ,  29030.76351224,\n",
       "        58341.77179355,  54254.44402825,  43760.27340861,  54205.57694463,\n",
       "        30071.97965402,  31068.20432795,  23694.97329392,  29188.2863522 ,\n",
       "        58407.86408354,  54379.64529963,  44377.29135971,  54946.19315653,\n",
       "         4583.28437805,   4593.09387207,   4599.13635254,   4602.96630859,\n",
       "         4590.02590179,   4590.67821503,   4604.3381691 ,   8633.20159912,\n",
       "         8647.78327942,   8653.63407135,   8657.10353851,   8567.41809845,\n",
       "         8563.95149231,   8650.61092377,   4284.31797028,   4294.00873184,\n",
       "         4300.35161972,   4304.56352234,   4306.54430389,   4307.93857574,\n",
       "         4307.86418915,   8716.837883  ,   8731.6160202 ,   8736.21559143,\n",
       "         8739.23492432,   8669.09408569,   8664.03102875,   8730.61084747])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31515.88092395,  31491.97439733,  23545.43853016,  28696.15215692,\n",
       "        56512.09549839,  55691.98399363,  43935.04051561,  53045.28383655,\n",
       "        29800.7306885 ,  31581.57944586,  23904.49425578,  28897.48649206,\n",
       "        56331.8435424 ,  55727.65622614,  44621.87614432,  53795.45261944,\n",
       "         4662.03403473,   4668.65444183,   4673.36845398,   4676.78642273,\n",
       "         4679.8210144 ,   4681.73456192,   4682.18040466,   8585.28137207,\n",
       "         8597.00584412,   8605.26847839,   8610.99529266,   8616.36447906,\n",
       "         8619.5640564 ,   8620.23258209,   4357.69128799,   4364.48669434,\n",
       "         4369.53306198,   4373.45981598,   4376.99556351,   4379.50801849,\n",
       "         4380.57947159,   8644.66667175,   8656.30531311,   8664.42012787,\n",
       "         8669.92473602,   8675.10318756,   8678.05480957,   8678.4734726 ])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pred1\n",
    "array([ 27641.75248798,  29371.85864616,  25570.17580885,  31332.33059663,\n",
    "        54520.74443363,  51845.12908466,  48774.34410434,  55326.67766046,\n",
    "        25941.31433219,  29007.37281621,  26055.36159314,  30610.44438742,\n",
    "        54624.25632589,  51950.94948448,  48839.72358517,  54025.13398416,\n",
    "         4083.06121826,   4151.80826187,   4202.17037201,   4237.41531372,\n",
    "         4283.16354752,   4294.18849945,   4287.25671768,   8383.12339783,\n",
    "         8432.74879456,   8463.34552765,   8478.36303711,   8533.41579437,\n",
    "         8524.24526215,   8443.73893738,   3734.3287468 ,   3804.32724953,\n",
    "         3855.49736023,   3892.77052879,   3934.67497826,   3949.16558266,\n",
    "         3958.9073658 ,   8184.02957916,   8267.44556427,   8323.92692566,\n",
    "         8358.41369629,   8430.58204651,   8431.64634705,   8367.65480042])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提交结果记录\n",
    "- 对于原始数据，无特殊说明都为取对数操作\n",
    "- 对于周预测模型，没有特殊指明的都为累计模型\n",
    "- 对于预测时得训练数据，周预测为20,天预测为100,最佳\n",
    "- 对于合并稀疏样本，无特殊说明，合并阈值为500，步长50\n",
    "- 对于特征数目，无特殊说明，则都为(2,21,2) (1,11) (1,31)\n",
    "- 73分记录，天预测特征266。，周预测特征210, sum\n",
    "- 77.19记录，新增类别特征，天预测特征267。，周预测特征211, mean，数值缩小1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- socre-52, 12/255, 周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- socre-50, 12/255, 周预测num_days=30, 天预测num_days=200, 回归模型，无正则化项\n",
    "- socre-41, 15/255, 周预测num_days=30, 天预测num_days=200, 回归模型，有正则化项alpha=5,lambda=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-50, 12/256, 周预测num_days=15, 天预测num_days=50, 回归模型，无正则化项\n",
    "- score-39, 15/256, 周预测使用1周1个预测模型，num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-53, 13/258, 周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-42, 17/259, 数据标准化处理，周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-40, 17/259, 数据标准化处理，周预测num_days=30, 天预测num_days=200, 回归模型，无正则化项\n",
    "- score-67, 12/261, 合并稀疏数据样本，周预测num_days=30, 天预测num_days=100, 回归模型，无正则化项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-08\n",
    "- score-66, 12/262, 合并稀疏数据样本，周预测num_days=30, 天预测num_days=100, 树模型，无正则化项\n",
    "- score-63, 12/262, 合并稀疏数据样本，周预测num_days=30, 天预测num_days=200, 回归模型，无正则化项\n",
    "- score-64, 12/262, 合并稀疏数据样本<=300样本、步长30，周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-68, 12/262, 合并稀疏数据样本，周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-62, 13/262, 合并稀疏数据样本<=500、步长25，周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-73, 12/264, 合并稀疏数据样本<=500、步长100，周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项\n",
    "- score-71, 13/264, 合并稀疏数据样本<=500、步长100,周预测num_days=20, 天预测num_days=100,树模型max_depth=7,5/gamma=0.1,0.1\n",
    "- score-72, 13/264, 合并稀疏数据样本<=500、步长100，特征数目减少8/6, 周预测num_days=20, 天预测num_days=100, 回归模型，无正则化项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-67, 13/264, 天预测合并稀疏数据样本<=500、步长100，天预测num_days=100;周预测合并稀疏数据样本<=150、步长50，周预测num_days=20 回归模型，无正则化项\n",
    "- score-73, 12/264, 天预测合并稀疏数据样本<=500、步长100，天预测num_days=100;周预测合并稀疏数据样本<=150、步长50，周预测num_days=20,周预测结果*1.8 回归模型，无正则化项\n",
    "- score-71, 13/264, 合并稀疏数据样本<=500、步长100,周预测num_days=20, 天预测num_days=100,树模型max_depth=10,8/gamma=0.1,0.1,周预测*1.3\n",
    "- score-70, 14/264, 天预测合并稀疏数据样本<=500、步长100，天预测num_days=100;周预测合并稀疏数据样本<=200、步长100，周预测num_days=20, 回归模型，无正则化项.周预测*1.3， 73\n",
    "- score-69, 14/264, 天预测合并稀疏数据样本<=500、步长50，天预测num_days=100;周预测合并稀疏数据样本<=400、步长200，周预测num_days=20, 回归模型，无正则化项.样本数减少，预测值会提高。\n",
    "- score-69, 14/264, 天预测合并稀疏数据样本<=500、步长50，天预测num_days=100;周预测合并稀疏数据样本<=400、步长200，周预测num_days=20, 回归模型，无正则化项.样本数减少，预测值会提高。 去掉无用特征，例如求和特征，提高衰减比例0.8/0.85\n",
    "- score-72, 12/264, 天预测合并稀疏数据样本，天预测num_days=100，周预测num_days=20, 回归模型，无正则化项。样本数减少，预测值会提高。 去掉无用特征，例如求和特征，提高衰减比例0.8/0.85\n",
    "- score-72, 12/264, 天预测合并稀疏数据样本，天预测num_days=100，周预测num_days=20, 回归模型，无正则化项。样本数减少，预测值会提高。 去掉无用特征，例如求和特征，减小衰减比例0.95\n",
    "- score-69, 12/264, 天预测合并稀疏数据样本<=500、步长50，天预测num_days=100;周预测合并稀疏数据样本<=500、步长100，周预测num_days=20。 去掉无用特征，例如求和特征 回归模型，无正则化项\n",
    "- score-72.54, 14/264, 合并稀疏数据样本，天预测num_days=100,周预测num_days=20。 去掉无用特征，例如求和特征 回归模型，无正则化项\n",
    "- score-72.16, 14/264, 合并稀疏数据样本，天预测num_days=100,周预测num_days=20。 回归模型，无正则化项.特征保留，部分sum替换未mean.使用全部数据训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-70.65, 15/266, 合并稀疏样本数据，取差分，预测结果周>天，天预测值略有减小。周数据先求和再取对数\n",
    "- score-71.88, 14/265, 增加特征\n",
    "- score-73.32, 12/265, 增加产品类型一个特征。\n",
    "- score-68.886, 16/265, 增加产品类型一个特征+差分预测，效果下降\n",
    "- score-77.19, 11/265, 增加产品类型一个特征，所有数值/1000预测，预测完成后×1000,关键在于数值浮动不能太大\n",
    "- score-77.14, 11/265, 无增加特征，所有数值/1000预测，预测完成后×1000,关键在于数值浮动不能太大\n",
    "- score-77.71, 10/266, 无增加特征，所有数值/1000预测，预测完成后×1000,合并步长800-100\n",
    "- score-77.81, 10/266, 去掉一堆无效特征,周预测剩余140，天预测剩余196, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长800-100\n",
    "- score-76.46, 13/266, 周预测剩余特征60,天预测剩余特征116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-11\n",
    "- ‘first_sales_days_in_last’和‘sale_count’两个特征很重要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-77.79, 10/266, 增加产品类型一个特征,去掉无效特征70个,周预测剩余140，天预测剩余196, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长800-200\n",
    "- score-66.76, 10/266, 增加产品类型一个特征,新增特征20个，去掉无效特征50个,周预测剩余111，天预测剩余167, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长800-200\n",
    "- score-77.84, 10/266, 去掉无效特征70个,周预测剩余140，天预测剩余196, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-77.81, 10/266, 去掉无效特征70个,周预测剩余140，天预测剩余196, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长1000-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-77.25, 13/279, 对原始数据做滑动平滑原span=3,此时数据样本未630条不是37条; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-76.56, 16/279, 对合并后数据做滑动平滑span=2,此时数据样本37条; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-76.56, 16/279, 对合并后数据做滑动平滑天数据span=4; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-76.44, 16/279, 对合并后数据做对数再取滑动平滑天数据span=4; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-77.48, 16/279, 对合并后数据做取滑动平滑天数据span=10; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-77.58, 16/279, 对合并后数据做取滑动平滑天数据span=30; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "- score-76.98, 16/279, 对合并后数据做取滑动平滑天数据span=180; 去掉无效特征70个,周预测剩余140+1，天预测剩余196+1, median/std/max/min,所有数值/1000预测，预测完成后×1000,合并步长500-100\n",
    "\n",
    "- score-78.14, 11/279, 对原始数据操作顺序：ewm(span=60)->merge(500,100)->/1000->log1p.去掉无效特征70个,周预测剩余140+1，天预测剩余196+1\n",
    "- score-74.44, 14/279, 对原始数据操作顺序：ewm(span=7)->merge(1000,100)->/1000->log1p.去掉无效特征70个,周预测剩余140+1，天预测剩余196+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-72.47, 19/280, 对原始数据操作顺序：ewm(span=7)->merge(500,250)->/1000->log1p,特征数目(614,608).\n",
    "- score-77.43, 13/281, 对原始数据操作顺序：ewm(span=120)->merge(500,250)->/1000->log1p,特征数目(614,608).\n",
    "- score-77.36, 13/281, 对原始数据操作顺序：ewm(span=120)->merge(500,100)->/1000->log1p,特征数目(614,608).\n",
    "- score-78.06, 10/281, 对原始数据操作顺序：ewm(span=60)->merge(500,100)->/1000->log1p,特征数目(620,614).\n",
    "- score-78.20, 10/281, 对原始数据操作顺序：ewm(span=30)->merge(500,100)->/1000->log1p,特征数目(620,614).\n",
    "- score-78.06, 10/281, 对原始数据操作顺序：ewm(span=15)->merge(500,100)->/1000->log1p,特征数目(620,614).\n",
    "span = 37是个好的尝试\n",
    "- score-77.87, 10/285, 尝试选取最好得三次结果取平均"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018-12-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- score-78.107, 10/291. 对原始数据操作顺序：ewm(span=37)->merge(500,100)->/1000->log1p,特征数目(620,614)\n",
    "- score-78.113, 10/291. 对原始数据操作顺序：ewm(span=30)->merge(500,100)->/1000->log1p,特征数目(620,614)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**观察储量趋势线，发现储量存在上升趋势，xgboost没有很好捕捉上升趋势，下一步采用神经网络模型。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "764px",
    "left": "309px",
    "top": "223.5px",
    "width": "220px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
